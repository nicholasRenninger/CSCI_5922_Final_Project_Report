Thus, in this project I am studying how RNN language models used as policy representations in a deep reinforcement learning setting fair as targets for state machine extraction. There is not much work in this specific field at all, but our approach would likely be based off of current work to extract state machines from an RNN representing the policy in the case of either model-based POMDP policy iteration \cite{Carr2019} \cite{Carr2020} or model-free reinforcement learning \cite{Koul2019}.

In this case, I will be interested in implementing the work done by Koul \cite{Koul2019} (in the \href{https://stable-baselines.readthedocs.io/}{stable-baselines} RL ecosystem), as this was the original work on extracting a moore-machine from an RNN encoding an RL agent's policy. This high-level approach is really interesting:

\begin{enumerate}
    \item Use a baseline RL algorithm (e.g. DDPG, A3C, PPO) to train a CNN-LSTM policy for an atari game given as an RL environment. This would involve designing a good CNN and LSTM architecture that trains well for the given environment.
    
    \item Next, we build the state machine \textit{into} the policy network by quantizing both the feature space of the LSTM input and the hidden state space of the LSTM into a small, finite number of discrete states. This is accomplished by training two autoencoders -- one for the feature quantization and one for the hidden state quantization. These autoencoders are trained by feeding features and traces from the replay buffer through these compression networks and minimizing the reconstruction loss given only binary or tenrary latent states.
    
    \item Then, these autoencoder pairs (called quantized bottleneck networks (QBNs)) are inserted back into the original policy network before and after the recurrent layers. As the quantization networks have already been trained, only fine tuning of this new policy network should be necessary with more RL episodic training.
    
    \item You now have what they call a moore-machine network, which is a neural network moore machine that represents the agent's policy. This is the state machine that we have been looking for!
\end{enumerate}

As our learning target is now a type of moore machine, let's define one. A moore machine is a finite, deterministic input / output state machine. For the Atari game pong, we can see a classical moore machine controller for it in Figure \ref{fig: pong_moore_machine}. A moore machine network works by taking a continuous observation of the world, and using its first QBN to encode this into some discrete state (think of the latent state of the QBN as being a binary / ternary encoding of the input). This discrete state is decoded and re-encoded by the rest of the LSTM and second QBN into a discrete hidden state - the state of the moore machine. This state is then decoded into an action, and then from the current hidden state the machine receives its next observation and the process continues. Hopefully it is clear now that the network in this case is being forced to act like a discrete state machine, despite being originally a purely continuous transformations of observations into actions.

So our major goal for this project is to pick a reinforcement learning environment studied by Koul et. al. and then try to learn a CNN-LSTM policy, two QBNs (one for observation features and one for hidden state), and a moore machine network policy to control. This involves migrating the lightweight research code for the Koul et. al. paper and integrating the custom models and training procedures into the stable-baselines, and thus for TF1.15. Having this implementation available will allow for the eventual extraction and learning of a finite state moore machine controller from the moore machine network.

Another large goal of this entire project was to build a reproducible environment for studying various deep reinforcement learning techniques. As such, all of this work was built with a custom docker environment that serves jupyter and tensorboard servers over the host's network. All of this is contained in my \href{https://github.com/nicholasRenninger/NeuralMooreMachine_Experiments}{github repo}. This container and its jupyter / tb servers can also be deployed on a remote cluster and then run through ssh on the remote machine - indeed this is how this work was all developed. The github repo contains comprehensive documentation on this project, including installation and usage. Also, all of the results here are contained in a very large, detailed jupyter notebook that walks through all of the steps to solve this problem in explicit detail. Along with this, all of the implementations I did are fully integrated with stable-baselines (the largest and most comprehensive RL library) and have been migrated from PyTorch to TF1.15 (tensor library underpinning stable-baselines), which should allow the work done Koul et. al. to be more widely applied to the vast RL ecosystem around stable-baselines. Thus, the advancement made here is largely on the implementation side. Check out the repo to get all of the documentation for the environment.
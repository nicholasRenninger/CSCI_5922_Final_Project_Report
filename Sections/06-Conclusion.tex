As we saw in the Results section, the CNN-LSTM and MMN agent both performed very well and were eventually found to be quite trainable. There is also the positive result that Koul et. al.'s work turned out to be quite reproduceable, a rarity in the world of AI research. The idea of QBN insertion is a potentially very powerful one, but needs more investigation. I am immediately thinking of using it along with power deep learning based feature extractors to help symbolize continuous data I have into symbols that I can use for specification learning. THe only problem with this idea is that this quantization step was not really well compared with any other way of doing feature compression and quantization. However, as with most things in deep learning, it is likely that deep feature representations will be more powerful and easier to construct than hand-designed features, so overall the results look promising. 

I think a nice extension of this work would be to apply this moore machine creation to imitation learning paradigms, so you can learn an explainable controller from just system demonstrations. Another interesting thing to try would be to try to learn other types of state-machines using this method, as I don't typically use moore machines in my research.